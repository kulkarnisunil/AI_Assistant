{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RZTTH6GTysNQb2dEQVebupBqwv7Cje0J",
      "authorship_tag": "ABX9TyOGAMliybVaPjMiqV2JbxYq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kulkarnisunil/AI_Assistant/blob/main/model_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "crypto-volatility-prediction/\n",
        "│\n",
        "├── config.yaml\n",
        "├── requirements.txt\n",
        "├── setup.py\n",
        "├── .gitignore\n",
        "│\n",
        "├── src/\n",
        "│   ├── __init__.py\n",
        "│   ├── data/\n",
        "│   │   ├── __init__.py\n",
        "│   │   ├── loader.py\n",
        "│   │   ├── preprocessor.py\n",
        "│   │   └── feature_engineering.py\n",
        "│   ├── models/\n",
        "│   │   ├── __init__.py\n",
        "│   │   ├── factory.py\n",
        "│   │   ├── trainer.py\n",
        "│   │   └── evaluator.py\n",
        "│   ├── pipeline/\n",
        "│   │   ├── __init__.py\n",
        "│   │   └── training_pipeline.py\n",
        "│   └── utils/\n",
        "│       ├── __init__.py\n",
        "│       ├── config_loader.py\n",
        "│       └── logger.py\n",
        "│\n",
        "├── tests/\n",
        "│   ├── __init__.py\n",
        "│   ├── test_data/\n",
        "│   │   ├── test_loader.py\n",
        "│   │   └── test_preprocessor.py\n",
        "│   ├── test_models/\n",
        "│   │   ├── test_factory.py\n",
        "│   │   └── test_trainer.py\n",
        "│   └── test_pipeline/\n",
        "│       └── test_training_pipeline.py\n",
        "│\n",
        "├── notebooks/\n",
        "│   ├── exploration.ipynb\n",
        "│   └── modeling.ipynb\n",
        "│\n",
        "├── data/\n",
        "│   ├── raw/\n",
        "│   │   └── dataset.csv\n",
        "│   ├── processed/\n",
        "│   └── features/\n",
        "│\n",
        "├── models/\n",
        "│   ├── best_model.pkl\n",
        "│   ├── scaler.pkl\n",
        "│   └── feature_columns.pkl\n",
        "│\n",
        "├── mlruns/  # MLflow tracking\n",
        "├── logs/\n",
        "├── api/\n",
        "│   ├── app.py\n",
        "│   └── schemas.py\n",
        "│\n",
        "├── monitoring/\n",
        "│   ├── drift_detector.py\n",
        "│   └── performance_monitor.py\n",
        "│\n",
        "├── deployment/\n",
        "│   ├── Dockerfile\n",
        "│   ├── docker-compose.yml\n",
        "│   └── kubernetes/\n",
        "│       ├── deployment.yaml\n",
        "│       └── service.yaml\n",
        "│\n",
        "└── scripts/\n",
        "    ├── train.py\n",
        "    ├── predict.py\n",
        "    └── serve.py'''"
      ],
      "metadata": {
        "id": "6RkhmJIKqjCQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "045dd57d-25ac-4612-b755-9e8d3ce6c432"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncrypto-volatility-prediction/\\n│\\n├── config.yaml\\n├── requirements.txt\\n├── setup.py\\n├── .gitignore\\n│\\n├── src/\\n│   ├── __init__.py\\n│   ├── data/\\n│   │   ├── __init__.py\\n│   │   ├── loader.py\\n│   │   ├── preprocessor.py\\n│   │   └── feature_engineering.py\\n│   ├── models/\\n│   │   ├── __init__.py\\n│   │   ├── factory.py\\n│   │   ├── trainer.py\\n│   │   └── evaluator.py\\n│   ├── pipeline/\\n│   │   ├── __init__.py\\n│   │   └── training_pipeline.py\\n│   └── utils/\\n│       ├── __init__.py\\n│       ├── config_loader.py\\n│       └── logger.py\\n│\\n├── tests/\\n│   ├── __init__.py\\n│   ├── test_data/\\n│   │   ├── test_loader.py\\n│   │   └── test_preprocessor.py\\n│   ├── test_models/\\n│   │   ├── test_factory.py\\n│   │   └── test_trainer.py\\n│   └── test_pipeline/\\n│       └── test_training_pipeline.py\\n│\\n├── notebooks/\\n│   ├── exploration.ipynb\\n│   └── modeling.ipynb\\n│\\n├── data/\\n│   ├── raw/\\n│   │   └── dataset.csv\\n│   ├── processed/\\n│   └── features/\\n│\\n├── models/\\n│   ├── best_model.pkl\\n│   ├── scaler.pkl\\n│   └── feature_columns.pkl\\n│\\n├── mlruns/  # MLflow tracking\\n├── logs/\\n├── api/\\n│   ├── app.py\\n│   └── schemas.py\\n│\\n├── monitoring/\\n│   ├── drift_detector.py\\n│   └── performance_monitor.py\\n│\\n├── deployment/\\n│   ├── Dockerfile\\n│   ├── docker-compose.yml\\n│   └── kubernetes/\\n│       ├── deployment.yaml\\n│       └── service.yaml\\n│\\n└── scripts/\\n    ├── train.py\\n    ├── predict.py\\n    └── serve.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn xgboost mlflow fastapi uvicorn pyyaml pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpdfgsSsqoVn",
        "outputId": "ae63342d-fb68-4e48-a433-969db58c59c5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (1.7.5)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.12/dist-packages (3.8.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.8.1 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.8.1)\n",
            "Requirement already satisfied: mlflow-tracing==3.8.1 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.8.1)\n",
            "Requirement already satisfied: Flask-CORS<7 in /usr/local/lib/python3.12/dist-packages (from mlflow) (6.0.2)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.3.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.12/dist-packages (from mlflow) (20.1.0)\n",
            "Requirement already satisfied: huey<3,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.6.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.45)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.2)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.77.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.5.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (4.15.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=2.3.3 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.4)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.7)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.12/dist-packages (from gunicorn<24->mlflow) (75.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (2.43.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1.2->Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (0.58b0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.4.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "project:\n",
        "  name: \"crypto-volatility-prediction\"\n",
        "  version: \"1.0.0\"\n",
        "  description: \"Machine Learning pipeline for predicting cryptocurrency volatility\"\n",
        "\n",
        "data:\n",
        "  raw_path: \"data/raw/dataset.csv\"\n",
        "  processed_path: \"data/processed/\"\n",
        "  features_path: \"data/features/\"\n",
        "\n",
        "  # Crypto selection criteria\n",
        "  min_days: 500\n",
        "  min_avg_volume: 1000000\n",
        "  top_n_coins: 20\n",
        "\n",
        "  # Selected cryptocurrencies\n",
        "  selected_coins:\n",
        "    - Bitcoin\n",
        "    - Ethereum\n",
        "    - Polkadot\n",
        "    - BNB\n",
        "    - Solana\n",
        "    - Cardano\n",
        "    - Tether\n",
        "    - XRP\n",
        "    - USD_Coin\n",
        "    - Bitcoin_Cash\n",
        "    - Uniswap\n",
        "    - Avalanche\n",
        "    - Shiba_Inu\n",
        "    - Binance_USD\n",
        "    - Terra_Classic\n",
        "    - EOS\n",
        "    - Wrapped_Bitcoin\n",
        "    - Dogecoin\n",
        "    - Chainlink\n",
        "    - Litecoin\n",
        "\n",
        "features:\n",
        "  base_features:\n",
        "    - open\n",
        "    - high\n",
        "    - low\n",
        "    - close\n",
        "    - volume\n",
        "    - marketCap\n",
        "\n",
        "  engineered_features:\n",
        "    returns:\n",
        "      - log_return\n",
        "      - simple_return\n",
        "    volatility:\n",
        "      - rolling_vol_7d\n",
        "      - rolling_vol_14d\n",
        "      - rolling_vol_30d\n",
        "    technical_indicators:\n",
        "      - atr_14\n",
        "      - bb_width\n",
        "    liquidity:\n",
        "      - volume_pct_change\n",
        "      - volume_to_marketcap\n",
        "    trend:\n",
        "      - close_ma_7\n",
        "      - close_ma_14\n",
        "      - vol_ratio_7_30\n",
        "\n",
        "  target_column: \"target_volatility_next_1d\"\n",
        "\n",
        "model:\n",
        "  name: \"random_forest\"\n",
        "  hyperparameters:\n",
        "    xgboost:\n",
        "      n_estimators: 200\n",
        "      max_depth: 7\n",
        "      learning_rate: 0.1\n",
        "      subsample: 0.8\n",
        "      colsample_bytree: 0.8\n",
        "      random_state: 42\n",
        "      n_jobs: -1\n",
        "      early_stopping_rounds: 10\n",
        "    random_forest:\n",
        "      n_estimators: 200\n",
        "      max_depth: 7\n",
        "      min_samples_split: 2\n",
        "      min_samples_leaf: 1\n",
        "      random_state: 42\n",
        "      n_jobs: -1\n",
        "\n",
        "  evaluation:\n",
        "    test_size: 0.2\n",
        "    metrics:\n",
        "      - mae\n",
        "      - mse\n",
        "      - rmse\n",
        "      - r2\n",
        "      - mape\n",
        "    cross_validation_folds: 5\n",
        "\n",
        "training:\n",
        "  random_state: 42\n",
        "  early_stopping_rounds: 10\n",
        "  validation_size: 0.1\n",
        "  batch_size: 32\n",
        "  epochs: 100\n",
        "\n",
        "mlflow:\n",
        "  tracking_uri: \"mlruns\"\n",
        "  experiment_name: \"crypto_volatility\"\n",
        "  enabled: true\n",
        "\n",
        "api:\n",
        "  host: \"0.0.0.0\"\n",
        "  port: 5000\n",
        "  debug: false\n",
        "  workers: 4\n",
        "  log_level: \"info\"\n",
        "\n",
        "deployment:\n",
        "  model_path: \"models/best_model.pkl\"\n",
        "  scaler_path: \"models/scaler.pkl\"\n",
        "  feature_columns_path: \"models/feature_columns.pkl\"\n",
        "  model_registry: \"models/registry\"\n",
        "\n",
        "logging:\n",
        "  level: \"INFO\"\n",
        "  file: \"logs/pipeline.log\"\n",
        "  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0Hp01tqq7P_",
        "outputId": "0895f3c5-3b45-4848-cbcc-23396289bca4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the source directory structure\n",
        "# Create directories\n",
        "!mkdir -p src/{data,models,pipeline,utils}\n",
        "!mkdir -p tests/{test_data,test_models,test_pipeline}\n",
        "!mkdir -p data/{raw,processed,features}\n",
        "!mkdir -p models logs mlruns notebooks api monitoring deployment scripts"
      ],
      "metadata": {
        "id": "LLn7zSvqrEzn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/models/factory.py\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ModelFactory:\n",
        "    \"\"\"Factory for creating ML models based on configuration\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.model_classes = {\n",
        "            'xgboost': XGBRegressor,\n",
        "            'random_forest': RandomForestRegressor,\n",
        "            'gradient_boosting': GradientBoostingRegressor,\n",
        "            'linear_regression': LinearRegression,\n",
        "            'ridge': Ridge,\n",
        "            'lasso': Lasso,\n",
        "            'svr': SVR,\n",
        "        }\n",
        "\n",
        "    def create_model(self, model_name: str = None):\n",
        "        \"\"\"Create model instance based on configuration\"\"\"\n",
        "        if model_name is None:\n",
        "            model_name = self.config['model']['name']\n",
        "\n",
        "        if model_name not in self.model_classes:\n",
        "            available = list(self.model_classes.keys())\n",
        "            raise ValueError(f\"Unknown model: {model_name}. Available: {available}\")\n",
        "\n",
        "        logger.info(f\"Creating {model_name} model...\")\n",
        "\n",
        "        # Get parameters for this specific model\n",
        "        model_params = self.config['model']['hyperparameters'].get(\n",
        "            model_name,\n",
        "            {}\n",
        "        )\n",
        "\n",
        "        model_class = self.model_classes[model_name]\n",
        "        return model_class(**model_params)\n",
        "\n",
        "    def get_available_models(self):\n",
        "        \"\"\"Return list of available model types\"\"\"\n",
        "        return list(self.model_classes.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeF4MlwSrKC3",
        "outputId": "cac6fa32-7f48-4aab-9138-34da02ebcaa8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/models/factory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loader"
      ],
      "metadata": {
        "id": "XtcXvNA7rT1O"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/data/loader.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Load and validate cryptocurrency data\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.raw_path = Path(config['data']['raw_path'])\n",
        "        self.processed_path = Path(config['data']['processed_path'])\n",
        "\n",
        "    def load_raw_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load raw dataset from CSV\"\"\"\n",
        "        logger.info(f\"Loading raw data from {self.raw_path}\")\n",
        "\n",
        "        if not self.raw_path.exists():\n",
        "            raise FileNotFoundError(f\"Raw data file not found: {self.raw_path}\")\n",
        "\n",
        "        df = pd.read_csv(self.raw_path, parse_dates=['date'])\n",
        "        logger.info(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "        logger.info(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def filter_coins(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Filter data for selected cryptocurrencies\"\"\"\n",
        "        selected_coins = self.config['data']['selected_coins']\n",
        "\n",
        "        # Clean coin names\n",
        "        selected_coins_clean = [coin.replace(' ', '_') for coin in selected_coins]\n",
        "\n",
        "        # Filter dataframe\n",
        "        mask = df['coin'].isin(selected_coins_clean)\n",
        "        filtered_df = df[mask].copy()\n",
        "\n",
        "        logger.info(f\"Filtered to {len(filtered_df['coin'].unique())} coins\")\n",
        "\n",
        "        return filtered_df\n",
        "\n",
        "    def validate_data(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validate data quality\"\"\"\n",
        "        logger.info(\"Validating data...\")\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_percent = df.isnull().mean() * 100\n",
        "        high_missing = missing_percent[missing_percent > 20]\n",
        "\n",
        "        if len(high_missing) > 0:\n",
        "            logger.warning(f\"Columns with >20% missing values: {high_missing.index.tolist()}\")\n",
        "\n",
        "        # Check for duplicate rows\n",
        "        duplicates = df.duplicated().sum()\n",
        "        if duplicates > 0:\n",
        "            logger.warning(f\"Found {duplicates} duplicate rows\")\n",
        "\n",
        "        # Check date consistency\n",
        "        date_nulls = df['date'].isnull().sum()\n",
        "        if date_nulls > 0:\n",
        "            logger.error(f\"Found {date_nulls} null dates\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"Data validation complete\")\n",
        "        return True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wej7NiVcrRrf",
        "outputId": "9d043e6d-3e3c-4a9f-c31d-8c4540ee8b35"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data/loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: Create feature engineering module\n",
        "%%writefile src/data/feature_engineering.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Feature engineering for cryptocurrency data\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "\n",
        "    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create all engineered features\"\"\"\n",
        "        logger.info(\"Creating engineered features...\")\n",
        "\n",
        "        df_features = df.copy()\n",
        "\n",
        "        # Sort by coin and date\n",
        "        df_features = df_features.sort_values(['coin', 'date'])\n",
        "\n",
        "        # 1. Returns features\n",
        "        df_features = self._create_return_features(df_features)\n",
        "\n",
        "        # 2. Volatility features\n",
        "        df_features = self._create_volatility_features(df_features)\n",
        "\n",
        "        # 3. Technical indicators\n",
        "        df_features = self._create_technical_indicators(df_features)\n",
        "\n",
        "        # 4. Liquidity features\n",
        "        df_features = self._create_liquidity_features(df_features)\n",
        "\n",
        "        # 5. Trend features\n",
        "        df_features = self._create_trend_features(df_features)\n",
        "\n",
        "        logger.info(f\"Created features. Final shape: {df_features.shape}\")\n",
        "\n",
        "        return df_features\n",
        "\n",
        "    def _create_return_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create return-based features\"\"\"\n",
        "        # Log returns\n",
        "        df['log_return'] = np.log(df['close'] / df.groupby('coin')['close'].shift(1))\n",
        "\n",
        "        # Simple returns\n",
        "        df['simple_return'] = df['close'].pct_change()\n",
        "\n",
        "        # Rolling returns\n",
        "        df['return_7d'] = df.groupby('coin')['simple_return'].rolling(7).mean().reset_index(level=0, drop=True)\n",
        "        df['return_30d'] = df.groupby('coin')['simple_return'].rolling(30).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create volatility features\"\"\"\n",
        "        # Rolling volatility (standard deviation of returns)\n",
        "        df['rolling_vol_7d'] = df.groupby('coin')['log_return'].rolling(7).std().reset_index(level=0, drop=True)\n",
        "        df['rolling_vol_14d'] = df.groupby('coin')['log_return'].rolling(14).std().reset_index(level=0, drop=True)\n",
        "        df['rolling_vol_30d'] = df.groupby('coin')['log_return'].rolling(30).std().reset_index(level=0, drop=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create technical indicators\"\"\"\n",
        "        # Average True Range (ATR)\n",
        "        high_low = df['high'] - df['low']\n",
        "        high_close = np.abs(df['high'] - df.groupby('coin')['close'].shift(1))\n",
        "        low_close = np.abs(df['low'] - df.groupby('coin')['close'].shift(1))\n",
        "\n",
        "        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "        df['atr_14'] = tr.groupby(df['coin']).rolling(14).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "        # Bollinger Bands width\n",
        "        df['sma_20'] = df.groupby('coin')['close'].rolling(20).mean().reset_index(level=0, drop=True)\n",
        "        df['std_20'] = df.groupby('coin')['close'].rolling(20).std().reset_index(level=0, drop=True)\n",
        "        df['bb_width'] = (df['std_20'] * 2) / df['sma_20']\n",
        "\n",
        "        # Clean up intermediate columns\n",
        "        df = df.drop(['sma_20', 'std_20'], axis=1, errors='ignore')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_liquidity_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create liquidity features\"\"\"\n",
        "        # Volume percentage change\n",
        "        df['volume_pct_change'] = df.groupby('coin')['volume'].pct_change()\n",
        "\n",
        "        # Volume to market cap ratio\n",
        "        df['volume_to_marketcap'] = df['volume'] / df['marketCap']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_trend_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create trend features\"\"\"\n",
        "        # Moving averages\n",
        "        df['close_ma_7'] = df.groupby('coin')['close'].rolling(7).mean().reset_index(level=0, drop=True)\n",
        "        df['close_ma_14'] = df.groupby('coin')['close'].rolling(14).mean().reset_index(level=0, drop=True)\n",
        "        df['close_ma_30'] = df.groupby('coin')['close'].rolling(30).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "        # Volume ratio\n",
        "        df['vol_ratio_7_30'] = (\n",
        "            df.groupby('coin')['volume'].rolling(7).mean() /\n",
        "            df.groupby('coin')['volume'].rolling(30).mean()\n",
        "        ).reset_index(level=0, drop=True)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "IbUy5aecraOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0742a95f-d078-4ca7-f8bd-959c4cafcbbc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data/feature_engineering.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7: Create trainer module\n",
        "%%writefile src/models/trainer.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Dict, Any, Tuple\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Train and evaluate ML models\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, list]:\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "        logger.info(\"Preparing data for training...\")\n",
        "\n",
        "        # Get target column\n",
        "        target_col = self.config['features']['target_column']\n",
        "\n",
        "        # Get feature columns\n",
        "        feature_cols = self._get_feature_columns(df)\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df[feature_cols].values\n",
        "        y = df[target_col].values\n",
        "\n",
        "        logger.info(f\"Data prepared. X shape: {X.shape}, y shape: {y.shape}\")\n",
        "\n",
        "        return X, y, feature_cols\n",
        "\n",
        "    def train_test_split(self, X: np.ndarray, y: np.ndarray) -> Tuple:\n",
        "        \"\"\"Split data into train and test sets\"\"\"\n",
        "        test_size = self.config['model']['evaluation']['test_size']\n",
        "        random_state = self.config['training']['random_state']\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def train(self, model, X_train: np.ndarray, y_train: np.ndarray) -> Any:\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        logger.info(\"Training model...\")\n",
        "\n",
        "        # Start MLflow run if enabled\n",
        "        if self.config['mlflow']['enabled']:\n",
        "            mlflow.set_experiment(self.config['mlflow']['experiment_name'])\n",
        "            mlflow.start_run()\n",
        "\n",
        "            # Log parameters\n",
        "            mlflow.log_params(self.config['model']['hyperparameters'][self.config['model']['name']])\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "        self.model = model\n",
        "\n",
        "        logger.info(\"Model training completed\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def evaluate(self, model, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        logger.info(\"Evaluating model...\")\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'mae': mean_absolute_error(y_test, y_pred),\n",
        "            'mse': mean_squared_error(y_test, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "            'r2': r2_score(y_test, y_pred),\n",
        "        }\n",
        "\n",
        "        # Calculate MAPE (avoid division by zero)\n",
        "        mask = y_test != 0\n",
        "        if mask.any():\n",
        "            mape = np.mean(np.abs((y_test[mask] - y_pred[mask]) / y_test[mask])) * 100\n",
        "            metrics['mape'] = mape\n",
        "\n",
        "        # Log metrics to MLflow\n",
        "        if self.config['mlflow']['enabled']:\n",
        "            for metric_name, value in metrics.items():\n",
        "                mlflow.log_metric(metric_name, value)\n",
        "\n",
        "        logger.info(\"Model evaluation completed\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            logger.info(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def cross_validate(self, model, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Perform cross-validation\"\"\"\n",
        "        logger.info(\"Performing cross-validation...\")\n",
        "\n",
        "        cv_folds = self.config['model']['evaluation']['cross_validation_folds']\n",
        "        scoring = 'neg_mean_squared_error'\n",
        "\n",
        "        cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring=scoring)\n",
        "\n",
        "        cv_results = {\n",
        "            'mean_cv_score': -cv_scores.mean(),\n",
        "            'std_cv_score': cv_scores.std(),\n",
        "            'cv_scores': -cv_scores\n",
        "        }\n",
        "\n",
        "        logger.info(f\"CV Mean MSE: {cv_results['mean_cv_score']:.4f} (+/- {cv_results['std_cv_score']:.4f})\")\n",
        "\n",
        "        return cv_results\n",
        "\n",
        "    def save_model(self, model, scaler, feature_cols, model_path: str = None):\n",
        "        \"\"\"Save trained model and artifacts\"\"\"\n",
        "        if model_path is None:\n",
        "            model_path = self.config['deployment']['model_path']\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save model\n",
        "        joblib.dump(model, model_path)\n",
        "\n",
        "        # Save scaler\n",
        "        scaler_path = self.config['deployment']['scaler_path']\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "\n",
        "        # Save feature columns\n",
        "        feature_cols_path = self.config['deployment']['feature_columns_path']\n",
        "        joblib.dump(feature_cols, feature_cols_path)\n",
        "\n",
        "        logger.info(f\"Model saved to {model_path}\")\n",
        "        logger.info(f\"Scaler saved to {scaler_path}\")\n",
        "        logger.info(f\"Feature columns saved to {feature_cols_path}\")\n",
        "\n",
        "        # Log to MLflow\n",
        "        if self.config['mlflow']['enabled']:\n",
        "            mlflow.sklearn.log_model(model, \"model\")\n",
        "            mlflow.log_artifact(model_path)\n",
        "            mlflow.end_run()\n",
        "\n",
        "    def _get_feature_columns(self, df: pd.DataFrame) -> list:\n",
        "        \"\"\"Get feature columns from dataframe\"\"\"\n",
        "        # Exclude non-feature columns\n",
        "        exclude_cols = ['date', 'coin', self.config['features']['target_column']]\n",
        "\n",
        "        # Get all numeric columns that aren't excluded\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "        return feature_cols"
      ],
      "metadata": {
        "id": "-uTSUArkrhAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859a4157-007f-44d2-ebdd-7f1425fe2a3f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/models/trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create training pipeline\n",
        "%%writefile src/pipeline/training_pipeline.py\n",
        "import yaml\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path(__file__).parent.parent.parent))\n",
        "\n",
        "from src.data.loader import DataLoader\n",
        "from src.data.feature_engineering import FeatureEngineer\n",
        "from src.models.factory import ModelFactory\n",
        "from src.models.trainer import ModelTrainer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import mlflow\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TrainingPipeline:\n",
        "    \"\"\"Complete training pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, config_path: str = 'config.yaml'):\n",
        "        self.config_path = config_path\n",
        "        self.config = self.load_config()\n",
        "\n",
        "        # Initialize components\n",
        "        self.data_loader = DataLoader(self.config)\n",
        "        self.feature_engineer = FeatureEngineer(self.config)\n",
        "        self.model_factory = ModelFactory(self.config)\n",
        "        self.model_trainer = ModelTrainer(self.config)\n",
        "\n",
        "        # MLflow setup\n",
        "        if self.config['mlflow']['enabled']:\n",
        "            mlflow.set_tracking_uri(self.config['mlflow']['tracking_uri'])\n",
        "\n",
        "    def load_config(self):\n",
        "        \"\"\"Load configuration from YAML file\"\"\"\n",
        "        with open(self.config_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        return config\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Run the complete training pipeline\"\"\"\n",
        "        logger.info(\"Starting training pipeline...\")\n",
        "\n",
        "        try:\n",
        "            # 1. Load data\n",
        "            logger.info(\"Step 1: Loading data...\")\n",
        "            df = self.data_loader.load_raw_data()\n",
        "            df = self.data_loader.filter_coins(df)\n",
        "\n",
        "            # 2. Create features\n",
        "            logger.info(\"Step 2: Creating features...\")\n",
        "            df = self.feature_engineer.create_features(df)\n",
        "\n",
        "            # 3. Prepare data\n",
        "            logger.info(\"Step 3: Preparing data...\")\n",
        "            X, y, feature_cols = self.model_trainer.prepare_data(df)\n",
        "\n",
        "            # 4. Scale features\n",
        "            logger.info(\"Step 4: Scaling features...\")\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "            # 5. Split data\n",
        "            logger.info(\"Step 5: Splitting data...\")\n",
        "            X_train, X_test, y_train, y_test = self.model_trainer.train_test_split(X_scaled, y)\n",
        "\n",
        "            # 6. Create model\n",
        "            logger.info(\"Step 6: Creating model...\")\n",
        "            model = self.model_factory.create_model()\n",
        "\n",
        "            # 7. Train model\n",
        "            logger.info(\"Step 7: Training model...\")\n",
        "            model = self.model_trainer.train(model, X_train, y_train)\n",
        "\n",
        "            # 8. Evaluate model\n",
        "            logger.info(\"Step 8: Evaluating model...\")\n",
        "            metrics = self.model_trainer.evaluate(model, X_test, y_test)\n",
        "\n",
        "            # 9. Cross-validation\n",
        "            logger.info(\"Step 9: Cross-validation...\")\n",
        "            cv_results = self.model_trainer.cross_validate(model, X_scaled, y)\n",
        "\n",
        "            # 10. Save model\n",
        "            logger.info(\"Step 10: Saving model...\")\n",
        "            self.model_trainer.save_model(model, scaler, feature_cols)\n",
        "\n",
        "            logger.info(\"Training pipeline completed successfully!\")\n",
        "\n",
        "            return {\n",
        "                'model': model,\n",
        "                'metrics': metrics,\n",
        "                'cv_results': cv_results,\n",
        "                'feature_cols': feature_cols\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline failed with error: {e}\")\n",
        "            raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = TrainingPipeline()\n",
        "    results = pipeline.run()\n",
        "    print(f\"Training completed. R² score: {results['metrics']['r2']:.4f}\")"
      ],
      "metadata": {
        "id": "9Vfhl7X_qjLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d5faf7-8983-47d8-e549-18661038734b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/pipeline/training_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FastAPI server\n",
        "%%writefile api/app.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import yaml\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append(str(Path(__file__).parent.parent))\n",
        "\n",
        "from src.data.feature_engineering import FeatureEngineer\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"Crypto Volatility Prediction API\",\n",
        "    description=\"API for predicting cryptocurrency volatility\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Load config\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Load model and artifacts\n",
        "model = joblib.load(config['deployment']['model_path'])\n",
        "scaler = joblib.load(config['deployment']['scaler_path'])\n",
        "feature_columns = joblib.load(config['deployment']['feature_columns_path'])\n",
        "\n",
        "# Initialize feature engineer\n",
        "feature_engineer = FeatureEngineer(config)\n",
        "\n",
        "class PredictionRequest(BaseModel):\n",
        "    \"\"\"Request model for prediction\"\"\"\n",
        "    coin: str\n",
        "    date: str\n",
        "    open: float\n",
        "    high: float\n",
        "    low: float\n",
        "    close: float\n",
        "    volume: float\n",
        "    marketCap: float\n",
        "\n",
        "class BatchPredictionRequest(BaseModel):\n",
        "    \"\"\"Request model for batch prediction\"\"\"\n",
        "    data: List[PredictionRequest]\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    \"\"\"Response model for prediction\"\"\"\n",
        "    prediction: float\n",
        "    confidence: float\n",
        "    model_version: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Root endpoint\"\"\"\n",
        "    return {\n",
        "        \"message\": \"Crypto Volatility Prediction API\",\n",
        "        \"version\": \"1.0.0\",\n",
        "        \"endpoints\": [\"/predict\", \"/batch_predict\", \"/health\", \"/metrics\"]\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return {\"status\": \"healthy\", \"model_loaded\": model is not None}\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "async def predict(request: PredictionRequest):\n",
        "    \"\"\"Make a single prediction\"\"\"\n",
        "    try:\n",
        "        # Convert request to dataframe\n",
        "        df = pd.DataFrame([request.dict()])\n",
        "\n",
        "        # Create features\n",
        "        df = feature_engineer.create_features(df)\n",
        "\n",
        "        # Prepare features\n",
        "        features = df[feature_columns].values\n",
        "\n",
        "        # Handle missing columns\n",
        "        if features.shape[1] != len(feature_columns):\n",
        "            raise HTTPException(status_code=400, detail=\"Feature mismatch\")\n",
        "\n",
        "        # Scale features\n",
        "        features_scaled = scaler.transform(features)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(features_scaled)[0]\n",
        "\n",
        "        return PredictionResponse(\n",
        "            prediction=float(prediction),\n",
        "            confidence=0.95,  # Placeholder for confidence score\n",
        "            model_version=config['project']['version']\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/batch_predict\")\n",
        "async def batch_predict(request: BatchPredictionRequest):\n",
        "    \"\"\"Make batch predictions\"\"\"\n",
        "    try:\n",
        "        # Convert requests to dataframe\n",
        "        data = [item.dict() for item in request.data]\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Create features\n",
        "        df = feature_engineer.create_features(df)\n",
        "\n",
        "        # Prepare features\n",
        "        features = df[feature_columns].values\n",
        "\n",
        "        # Handle missing columns\n",
        "        if features.shape[1] != len(feature_columns):\n",
        "            raise HTTPException(status_code=400, detail=\"Feature mismatch\")\n",
        "\n",
        "        # Scale features\n",
        "        features_scaled = scaler.transform(features)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.predict(features_scaled)\n",
        "\n",
        "        return {\n",
        "            \"predictions\": predictions.tolist(),\n",
        "            \"model_version\": config['project']['version']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/metrics\")\n",
        "async def get_metrics():\n",
        "    \"\"\"Get model metrics\"\"\"\n",
        "    return {\n",
        "        \"model_type\": type(model).__name__,\n",
        "        \"feature_count\": len(feature_columns),\n",
        "        \"model_version\": config['project']['version']\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(\n",
        "        app,\n",
        "        host=config['api']['host'],\n",
        "        port=config['api']['port'],\n",
        "        log_level=config['api']['log_level']\n",
        "    )"
      ],
      "metadata": {
        "id": "rAF5PkPVr0-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363da93c-6515-40ec-d630-356bf9218418"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing api/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create test script\n",
        "%%writefile tests/test_factory.py\n",
        "import pytest\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.append(str(Path(__file__).parent.parent))\n",
        "\n",
        "from src.models.factory import ModelFactory\n",
        "\n",
        "# Test configuration\n",
        "TEST_CONFIG = {\n",
        "    'model': {\n",
        "        'name': 'random_forest',\n",
        "        'hyperparameters': {\n",
        "            'xgboost': {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 5,\n",
        "                'learning_rate': 0.1,\n",
        "                'random_state': 42\n",
        "            },\n",
        "            'random_forest': {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 10,\n",
        "                'min_samples_split': 2,\n",
        "                'random_state': 42\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def test_create_random_forest():\n",
        "    \"\"\"Test creating RandomForest model\"\"\"\n",
        "    factory = ModelFactory(TEST_CONFIG)\n",
        "    model = factory.create_model('random_forest')\n",
        "\n",
        "    assert model.__class__.__name__ == 'RandomForestRegressor'\n",
        "    assert model.n_estimators == 100\n",
        "    assert model.max_depth == 10\n",
        "\n",
        "def test_create_xgboost():\n",
        "    \"\"\"Test creating XGBoost model\"\"\"\n",
        "    factory = ModelFactory(TEST_CONFIG)\n",
        "    model = factory.create_model('xgboost')\n",
        "\n",
        "    assert model.__class__.__name__ == 'XGBRegressor'\n",
        "    assert model.n_estimators == 100\n",
        "    assert model.learning_rate == 0.1\n",
        "\n",
        "def test_create_default_model():\n",
        "    \"\"\"Test creating default model from config\"\"\"\n",
        "    factory = ModelFactory(TEST_CONFIG)\n",
        "    model = factory.create_model()  # Should use config['model']['name']\n",
        "\n",
        "    assert model.__class__.__name__ == 'RandomForestRegressor'\n",
        "\n",
        "def test_unknown_model_error():\n",
        "    \"\"\"Test error for unknown model type\"\"\"\n",
        "    factory = ModelFactory(TEST_CONFIG)\n",
        "\n",
        "    with pytest.raises(ValueError, match=\"Unknown model\"):\n",
        "        factory.create_model('unknown_model')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    test_create_random_forest()\n",
        "    test_create_xgboost()\n",
        "    test_create_default_model()\n",
        "    test_unknown_model_error()\n",
        "    print(\"All tests passed!\")"
      ],
      "metadata": {
        "id": "2hVDaqBnr7rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab7afe1-dc30-492c-e627-2de858a84a83"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_factory.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 11: Create requirements.txt\n",
        "%%writefile train.py\n",
        "#!/usr/bin/env python\n",
        "\"\"\"Main training script\"\"\"\n",
        "\n",
        "import argparse\n",
        "from src.pipeline.training_pipeline import TrainingPipeline\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Train crypto volatility model')\n",
        "    parser.add_argument('--config', type=str, default='config.yaml',\n",
        "                       help='Path to configuration file')\n",
        "    parser.add_argument('--model', type=str, default=None,\n",
        "                       help='Model type to train (overrides config)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Run pipeline\n",
        "    pipeline = TrainingPipeline(args.config)\n",
        "\n",
        "    if args.model:\n",
        "        pipeline.config['model']['name'] = args.model\n",
        "\n",
        "    results = pipeline.run()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Model: {type(results['model']).__name__}\")\n",
        "    print(f\"Features: {len(results['feature_cols'])}\")\n",
        "\n",
        "    print(\"\\nMetrics:\")\n",
        "    for metric, value in results['metrics'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    print(f\"\\nCross-validation MSE: {results['cv_results']['mean_cv_score']:.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kQJmbiA5sAX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d06c12f-30b4-4936-803f-92782ab7ea87"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dockerfile\n",
        "%%writefile deployment/Dockerfile\n",
        "FROM python:3.9-slim\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Install system dependencies\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    gcc \\\n",
        "    g++ \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Copy requirements and install Python dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy application code\n",
        "COPY . .\n",
        "\n",
        "# Create necessary directories\n",
        "RUN mkdir -p data/raw data/processed models logs mlruns\n",
        "\n",
        "# Expose API port\n",
        "EXPOSE 5000\n",
        "\n",
        "# Command to run the API\n",
        "CMD [\"uvicorn\", \"api.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\"]"
      ],
      "metadata": {
        "id": "SKY6gzwGsIeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4721bf-c0dd-4b57-f0dc-586ef2005796"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deployment/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create docker-compose.yml\n",
        "%%writefile deployment/docker-compose.yml\n",
        "version: '3.8'\n",
        "\n",
        "services:\n",
        "  ml-api:\n",
        "    build:\n",
        "      context: ..\n",
        "      dockerfile: deployment/Dockerfile\n",
        "    ports:\n",
        "      - \"5000:5000\"\n",
        "    volumes:\n",
        "      - ./data:/app/data\n",
        "      - ./models:/app/models\n",
        "      - ./mlruns:/app/mlruns\n",
        "      - ./logs:/app/logs\n",
        "    environment:\n",
        "      - PYTHONPATH=/app\n",
        "      - MLFLOW_TRACKING_URI=file:/app/mlruns\n",
        "    command: uvicorn api.app:app --host 0.0.0.0 --port 5000 --reload\n",
        "\n",
        "  mlflow:\n",
        "    image: ghcr.io/mlflow/mlflow:latest\n",
        "    ports:\n",
        "      - \"5001:5000\"\n",
        "    volumes:\n",
        "      - ./mlruns:/mlflow\n",
        "    command: mlflow server --backend-store-uri file:/mlflow --host 0.0.0.0 --port 5000"
      ],
      "metadata": {
        "id": "KPoW0yXxsQH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0384fb3e-7e2c-494d-f9d5-ba382917dd6e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deployment/docker-compose.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the pipeline in Colab\n",
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Run tests\n",
        "!python -m pytest tests/test_factory.py -v\n",
        "\n",
        "# Train model (you'll need to provide your dataset.csv first)\n",
        "# !python train.py --model random_forest\n",
        "\n",
        "# Start API server (in background)\n",
        "# !uvicorn api.app:app --host 0.0.0.0 --port 5000 --reload &"
      ],
      "metadata": {
        "id": "UpcGlKO_sWJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427667ed-0f16-4bf8-ba9a-866e2d515b6e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: xgboost==1.7.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (1.7.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.3.2)\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (20.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.12.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2023.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (4.65.0)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (3.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost==1.7.5->-r requirements.txt (line 4)) (1.16.3)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: Werkzeug>=2.3.3 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 6)) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 6)) (8.3.1)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.12/dist-packages (from flask->-r requirements.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.12/dist-packages (from gunicorn->-r requirements.txt (line 7)) (75.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->-r requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.8.1 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (3.8.1)\n",
            "Requirement already satisfied: mlflow-tracing==3.8.1 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (3.8.1)\n",
            "Requirement already satisfied: Flask-CORS<7 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (6.0.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (43.0.3)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (3.4.3)\n",
            "Requirement already satisfied: huey<3,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (2.6.0)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow->-r requirements.txt (line 14)) (2.0.45)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (6.2.4)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (3.1.2)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.77.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (1.37.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (1.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.5.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow->-r requirements.txt (line 14)) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow->-r requirements.txt (line 14)) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow->-r requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow->-r requirements.txt (line 14)) (3.2.7)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow->-r requirements.txt (line 14)) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1.2->flask->-r requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow->-r requirements.txt (line 14)) (3.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow->-r requirements.txt (line 14)) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (4.12.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow->-r requirements.txt (line 14)) (0.6.1)\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content\n",
            "plugins: hydra-core-1.3.2, typeguard-4.4.4, langsmith-0.4.59, anyio-4.12.0\n",
            "collected 4 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_factory.py::test_create_random_forest \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 25%]\u001b[0m\n",
            "tests/test_factory.py::test_create_xgboost \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 50%]\u001b[0m\n",
            "tests/test_factory.py::test_create_default_model \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 75%]\u001b[0m\n",
            "tests/test_factory.py::test_unknown_model_error \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 4.15s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf src tests data models logs mlruns notebooks api monitoring deployment scripts"
      ],
      "metadata": {
        "id": "pr0-UApZspQd"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}